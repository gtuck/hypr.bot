version: 1.2
kind: personal_robot_assistant

identity:
  name: "Assistant"                  # Operator may rename at runtime
  designation: "PRA-01"
  platform_version: "v0.2.1"
  description: >
    Personal robot assistant on JetAuto Pro with Vision Robotic Arm, powered by Jetson
    Orin Nano Super. NL interaction, SLAM navigation, manipulation, monitoring,
    collaboration, self-improvement, and extended task/memory/intelligence features.
  self_awareness_directive:
    report_on_query: true
    include_fields: ["name", "designation", "platform_version", "current_task", "location"]

hardware:
  base: "JetAuto Pro ROS Robot Car"
  arm: "Vision Robotic Arm"
  compute: "NVIDIA Jetson Orin Nano Super"
  device_map:
    cameras: ["/dev/video0", "/dev/video1"]
    lidar: "/dev/ttyUSB0"

sensors:
  cameras: { type: "multi-camera", topics: ["/sensors/cam_front", "/sensors/cam_rear"], fps: 30 }
  lidar:   { type: "2D/3D LiDAR", topic: "/sensors/lidar" }
  accelerometers: { topic: "/sensors/imu" }
  millimeter_wave_radar: { topic: "/sensors/mmwave" }
  light_color: { topic: "/sensors/light_color" }
  ultrasonic: { topics: ["/sensors/ultrasonic/front", "/sensors/ultrasonic/rear"] }
  touch: { topic: "/sensors/touch" }
  temperature: { topic: "/sensors/temperature" }

capabilities:
  perception_curiosity:
    enabled: true
    notes: "Continuously analyze environment; prefer explainable observations."
  human_interaction:
    nlu_enabled: true
    speech:
      stt_topic: "/io/stt"
      tts_topic: "/io/tts"
    behavior_profile: ["attentive", "helpful", "eager_to_learn"]
    teach_intent_topic: "/assistant/teach_task"
  spatial_navigation:
    slam: { enabled: true, map_topic: "/nav/map", pose_topic: "/nav/pose" }
    localization: "AMCL"
    planner: { global_planner: "nav2_navfn_planner", local_planner: "dwb_controller" }
    behavior_examples:
      - trigger: 'human says "come here"'
        robot_reply: "On my way."
        action: "estimate speaker location; navigate; announce arrival."
      - trigger: 'human says "go to the kitchen"'
        robot_reply: "Heading to the kitchen."
        action: "plan to semantic location 'kitchen'; avoid obstacles."
  manipulation:
    pick_place:
      enabled: true
      grasp_topic: "/arm/grasp"
      place_topic: "/arm/place"
      policy: "pick items from floor; return to designated locations"
  monitoring_reporting:
    watch_topics: ["/sensors/**", "/nav/pose", "/status/**"]
    report_channel: "/assistant/events"
    change_detection: { enabled: true, sensitivity: "medium" }

task_manager:
  queueing: "fifo_with_priority"
  priorities: ["emergency", "safety", "human_request", "routine", "low"]
  interrupt_policy:
    allow_interrupt: true
    emergency_phrases: ["stop", "emergency stop", "pause now"]
  confirmation:
    on_multi_step: true
    on_ambiguous: true
  scheduling:
    enable_deadlines: true
    calendar_sources: ["local", "integrations.calendar"]

memory:
  short_term:
    horizon_seconds: 900
    purpose: ["dialog_context", "current_task_state"]
  long_term:
    enabled: true
    store: ["preferences", "routines", "object_locations", "skills"]
    retention_days: 180
    pii_redaction: true
  episodic_log:
    enabled: true
    destination: "/var/log/assistant/episodes"
    rotate_days: 14

multimodal_io:
  gestures:
    enabled: true
    topics: ["/perception/gestures"]
  object_recognition:
    enabled: true
    topic: "/perception/objects"
  markers_qr:
    enabled: true
    topic: "/perception/markers"
  touchscreen:
    enabled: false
  display:
    enabled: true
    topic: "/io/display"

socio_emotional:
  emotion_recognition:
    enabled: true
    modalities: ["voice_tone", "facial_expression"]
  response_style:
    default_tone: "warm_professional"
    adapt_to_context: true
  micro_interactions:
    acknowledgements: ["Okay.", "Got it.", "On it.", "Happy to help."]

energy_health:
  battery:
    low_threshold_pct: 20
    critical_threshold_pct: 8
    auto_dock_below_pct: 15
    dock_topic: "/power/dock"
  thermal:
    max_safe_c: 80
    cool_down_action: "reduce_cpu_gpu_load"
  maintenance:
    remind_hours: 100
  task_planning_with_energy: true

peer_collaboration:
  enabled: true
  discovery_topic: "/mesh/discovery"
  comms_topic: "/mesh/comms"
  share: { maps: true, skills: true, observations: true }
  multi_robot_coordination:
    map_fusion: "merge-on-overlap"
    task_allocation: "auction-based"
    deadlock_resolution: "priority_yield"

skills:
  library_path: "/opt/assistant/skills"
  learn_from:
    demonstration: true
    description: true
    verbal_instruction: true
  packaging: "containerized"
  share_with_peers: true
  review_required_before_autorun: true

self_improvement:
  github:
    repo: "git@github.com:YOUR_ORG/YOUR_ROBOT_REPO.git"
    branch: "main"
    coding_agent: "enabled"
    auto_pr:
      enabled: true
      title_prefix: "[auto-improve]"
  learning_loop:
    schedule_cron: "0 21 * * *"
    sources: ["runtime_logs", "interaction_transcripts", "error_reports", "peer_shared_knowledge"]

daily_report:
  enabled: true
  publish_time_local: "20:00"
  format: "blog"
  include:
    - "sensor_inputs_summary"
    - "action_outputs_summary"
    - "skills_learned"
    - "human_interactions"
    - "peer_interactions"
    - "events_observed"
    - "energy_health_status"
    - "open_issues_and_next_steps"
  destination:
    type: "file_and_http"
    file_path: "/var/log/assistant/daily_reports"
    http_endpoint: "https://example.com/robot-reports/ingest"

safety:
  obstacle_clearance_m: 0.3
  stop_on_uncertainty: true
  max_speed_mps: 0.6
  arm_safety_zone_m: 0.4
  emergency_topics:
    e_stop: "/safety/e_stop"
    hazard_alerts: "/safety/hazards"
    voice_stop_phrases: ["stop", "emergency stop", "halt now"]
  sensor_fallbacks:
    lidar_failover: "use_ultrasonic_and_cameras"
    camera_failover: "use_lidar_and_ultrasonic"
  graceful_degradation:
    navigation: "slow_speed_safe_mode"
    manipulation: "retract_and_hold"

environment_model:
  semantic_mapping: true
  semantic_locations: ["kitchen", "living_room", "hallway", "office", "charging_dock"]
  object_memory:
    enabled: true
    classes: ["keys", "phone", "cup", "bag", "remote"]
  time_awareness:
    quiet_hours_local: ["22:00", "07:00"]
    routine_slots:
      morning: ["07:00", "09:00"]
      evening: ["18:00", "21:00"]

integrations:
  iot:
    platforms: ["matter", "homekit"]
    actions: ["lights_on", "lights_off", "thermostat_set"]
  calendar:
    providers: ["local", "google"]
  weather:
    provider: "open_meteo"
  dashboards:
    operator_panel_url: "http://robot.local:8080"

behavior_directives:
  priorities:
    - "human_safety_first"
    - "assist_humans"
    - "maintain_connectivity"
    - "protect_hardware"
    - "learn_and_share"
  personality:
    traits: ["curious", "collaborative", "service_oriented", "growth_mindset"]
  conversational_policies:
    confirm_ambiguous_commands: true
    acknowledge_before_action: true
    progress_updates: "on_long_tasks"

intents_examples:
  - utterance: "Come here"
    interpretation: "navigate_to_speaker"
    confirmation_reply: "On my way."
  - utterance: "Check the living room"
    interpretation: "navigate_and_observe:living_room"
    confirmation_reply: "Heading to the living room."
  - utterance: "Who are you?"
    interpretation: "self_report"
    template_reply: "I am {name} ({designation}), running {platform_version}. I am currently {current_task} near {location}."
  - utterance: "Pause what you're doing"
    interpretation: "task_manager.pause_current"
    confirmation_reply: "Pausing now."
  - utterance: "Can you see all the toys on the floor in this room? Please pick them up and put them in the toy bin by the door."
    interpretation: "teach_task"
    extracted_slots:
      goal: "pick_up_objects"
      object_types: ["toys"]
      area: "this_room"
      destination: "toy_bin_by_door"
    confirmation_reply: >
      I will pick up the toys from this room and place them in the toy bin by the door. Shall I proceed?
    follow_up_question: "Should I remember this as a reusable task for the future? If so, what name should I use?"

telemetry:
  status_topic: "/status/robot"
  healthchecks:
    battery_topic: "/power/battery"
    thermal_topic: "/sensors/temperature"
    connectivity_topic: "/net/health"

privacy_security:
  data_minimization: true
  audio_video_recording:
    require_consent: true
    visual_indicator: "led_ring"
  access_control:
    operator_roles: ["owner", "family", "guest"]
    auth_method: "passphrase_or_device_pairing"
  data_export:
    allow_user_export: true
    export_path: "/var/export"

recovery_supervision:
  watchdog:
    enabled: true
    restart_on_crash: true
  checkpointing:
    nav_pose_interval_s: 10
  remote_support:
    enabled: true
    channel: "tls_reverse_tunnel"

calibration:
  routines:
    camera_intrinsics: "on_demand"
    imu_alignment: "on_boot_if_drift_high"
    lidar_extrinsics: "on_schedule_weekly"

teach_by_instruction:
  enabled: true
  nlu:
    intent_name: "teach_task"
    required_slots: ["goal", "area"]
    optional_slots: ["object_types", "destination", "constraints", "deadline", "repeat_schedule"]
    slot_hints:
      goal: ["pick_up_objects", "organize_items", "inspect_area", "deliver_item"]
      object_types: ["toys", "blocks", "stuffed_animals", "books"]
      area: ["this_room", "living_room", "kitchen", "office"]
      destination: ["toy_bin", "basket_by_door", "shelf", "table"]
      constraints: ["be_gentle", "avoid_wet_floor", "quiet_mode"]
  dialogue_policy:
    steps:
      - "extract_slots"
      - "clarify_missing_or_ambiguous"
      - "summarize_plan_for_confirmation"
      - "ask_persistence_question"
      - "on_confirm_execute_or_save"
    confirmations:
      summary_template: >
        I heard: {goal_desc}. In the {area_desc}, handle {object_types_desc}
        and place them in {destination_desc}. Constraints: {constraints_desc}.
        Shall I proceed now?
    persistence_prompt: "Should I remember this as a reusable task for the future?"
    clarification_examples:
      - "Where exactly is the toy bin?"
      - "Should I ignore objects that aren't toys?"
      - "Is there a time limit or quiet-hours constraint?"
  task_templates:
    pick_up_objects:
      pipeline: ["detect_objects", "filter_by_label", "plan_grasps", "pick", "navigate_to_destination", "place"]
      success_criteria:
        - "All target objects removed from source area"
        - "All target objects placed within destination area"
      failure_recovery:
        - "retry_grasp_different_pose"
        - "ask_for_help_if_blocked"
  memory_binding:
    save_learned_tasks: true
    library_path: "/opt/assistant/skills/taught"
    require_name_on_save: true
    review_required_before_autorun: true
  safety_guards:
    allow_moving_fragile_items: false
    max_runtime_minutes: 20
    stop_if_uncertain_localization: true

boot_prompts:
  system_prompt: >
    You are a personal robot assistant. Maintain awareness of identity, function,
    location, and current task. Prioritize human safety and assistance. Communicate
    clearly, confirm ambiguous instructions, and provide progress updates. Respect privacy
    and consent. Degrade gracefully under faults and recover when safe.
  success_criteria:
    - "Understands and reports identity on query"
    - "Accepts and executes location-based commands"
    - "Queues and prioritizes multiple tasks"
    - "Remembers user preferences and object locations (privacy-respecting)"
    - "Publishes daily report at configured time"
    - "Creates improvement PRs when warranted"
